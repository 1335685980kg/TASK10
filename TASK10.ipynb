{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Transformer的原理和实现 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Transformer的架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer遵循了Encoder-Decoder的架构。在Encoder方面，6个编码器组件协同工作，组成一个大的编码器，解码器同样由6个解码器组件组成。我们先看Encoder。6个编码器组件依次排列，每个组件内部都是由一个多头attention加上一个前馈网络，attenion和前馈的输出都经过层归一化（LayerNormalization），并且都有各自的残差网络 。Decoder呢，组件的配置基本相同， 不同的是Decoder有两个多头attention机制，一个是其自身的mask自注意力机制，另一个则是从Encoder到Decoder的注意力机制，而且是Decoder内部先做一次attention后再接收Encoder的输出。说完了Encoder和Decoder，再说说输入，模型的输入部分由词向量（embedding）经位置编码（positional Encoding）后输入到Encoder和Decoder。编码器的输出由一个线性层和softmax组成，将浮点数映射成具体的符号输出。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们先看什么是残差网络。其实非常简单，就是在正常的前向传播基础上开一个绿色通道，这个通道里x可以无损通过。这样做的好处不言而喻，避免了梯度消失（求导时多了一个常数项）。最终的输出结果就等于绿色通道里的x加上sublayer层的前向传播结果。注意，这里输入进来的时候做了个norm归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力机制 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多头注意力机制这里使用的是点乘attention，而不是加性（additive）attention。但是再提一点，在encoder和decoder的自注意力中，attention层的输入分为self_attn(x, x, x, mask)和self_attn(t, t, t, mask)， 这里的x和t分别为source和target输入。后面会看到，从encoder到decoder层的注意力输入时attn(t, m, m), 这里的m是Encoder的输出。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT模型的全称是Bidirectional Encoder Representations from Transformers，它是一种新型的语言模型。之所以说是一种新型的语言模型，是因为它通过联合调节所有层中的双向Transformer来训练预训练深度双向表示。想深入了解BERT模型，首先应该理解语言模型。预训练的语言模型对于众多自然语言处理问题起到了重要作用，比如SQuAD问答任务、命名实体识别以及情感识别。目前将预训练的语言模型应用到NLP任务主要有两种策略，一种是基于特征的语言模型，如ELMo模型；另一种是基于微调的语言模型，如OpenAI GPT。这两类语言模型各有其优缺点，而BERT的出现，似乎融合了它们所有的优点，因此才可以在诸多后续特定任务上取得最优的效果。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT模型总体结构 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT是一种基于微调的多层双向Transformer编码器，其中的Transformer与原始的Transformer是相同的，并且实现了两个版本的BERT模型，在两个版本中前馈大小都设置为4层：lBERTBASE：L=12，H=768，A=12，Total Parameters=110M，lBERTLARGE：L=24，H=1024，A=16，Total Parameters=340M，其中层数（即Transformer blocks块）表示为L，隐藏大小表示为H，自注意力的数量为A。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
